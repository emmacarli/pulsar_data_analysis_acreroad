{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSci thesis by 2185343, 2020 - see report for details.\n",
    "\n",
    "This is a summary version of all the code written for this project. Some scripts are left out for clarity, notably the FFT transform section, plotting scripts, Gaussian fitting of the template, and repeated steps. The whole project is available on GitHub (under my name)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For interactive plots, use:\n",
    "#%matplotlib ipympl\n",
    "\n",
    "# Import packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from progress.bar import Bar #https://pypi.org/project/progress/\n",
    "\n",
    "from astropy.stats import sigma_clip\n",
    "from astropy.time import Time\n",
    "from astropy import units\n",
    "from astropy.visualization import quantity_support\n",
    "\n",
    "\n",
    "from pint import models, toa, residuals, fitter\n",
    "from scipy.signal import resample\n",
    "\n",
    "\n",
    "from My_Functions.update_text import update_text #taken from https://stackoverflow.com/questions/49742962/how-to-insert-text-at-line-and-column-position-in-a-file\n",
    "from shutil import copyfile\n",
    "import os, re, time, subprocess, glob\n",
    "import fileinput\n",
    "\n",
    "\n",
    "# Make text larger for readability on graphs\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "plt.rcParams['font.size'] = 14\n",
    "# Set same fonts as my LaTeX document\n",
    "plt.rcParams['font.family'] = 'STIXGeneral' \n",
    "plt.rcParams['mathtext.fontset'] = 'stix'\n",
    "# Other plotting params\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams[\"figure.figsize\"] = [10,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set known variables from observations\n",
    "\n",
    "sampling_period = 2e-3 #2 milliseconds\n",
    "one_second_in_datapoints = int(1/sampling_period) #500 datapoints = 1 second of recording\n",
    "one_minute_in_datapoints= one_second_in_datapoints * 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths\n",
    "\n",
    "path_to_raw_files = '/home/emma/Desktop/Raw_Data/'\n",
    "path_to_cleaned_files = '/home/emma/Desktop/Cleaned_Data/' #these have been cleaned with my routine and not the previous one\n",
    "path_to_folded_profiles =  '/home/emma/Desktop/pulsar_data_analysis_acreroad/PRESTO_Folded_Profiles/'\n",
    "path_to_template_profile = '/home/emma/Desktop/pulsar_data_analysis_acreroad/Jodrell_Template_Profile_I-Q_PRESTO_Gaussian_fit.gaussians' #this was generated using PRESTO's pygaussfit.py on the Jodrell Bank template for this pulsar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files needed in the working directory: see README"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of raw files' paths in my computer\n",
    "raw_files = glob.glob(path_to_raw_files+'*-PSRB0329-2ms-sampling-dd.dat') #this suffix is added at recording\n",
    "\n",
    "bar = Bar('Processing...', max=len(raw_files), fill='\\U0001F4E1', suffix = '%(percent).1f%% - %(eta)ds') #create a progress bar\n",
    "bar.check_tty = False\n",
    "\n",
    "# Go through each raw file\n",
    "for raw_file in raw_files: \n",
    "    start_time_GPS = float(raw_file[len(path_to_raw_files):-len('-PSRB0329-2ms-sampling-dd.dat')]) #the start of the filename gives the beginning of the recording time in GPS format\n",
    "    bar.next()\n",
    "      \n",
    "    #If this file has already been cleaned, skip it and go to the next in raw_files\n",
    "    if glob.glob(path_to_cleaned_files+str(start_time_GPS)+'_cleaned.dat') != []:\n",
    "        print(str(start_time_GPS)+' already cleaned.')\n",
    "        continue\n",
    "    \n",
    "    # Load in the raw data\n",
    "    handle_raw_file = open(raw_file, mode='rb') \n",
    "    data_raw = np.fromfile(handle_raw_file,'f4')\n",
    "    \n",
    "    # Crop the raw file to a length that is an integer number of minutes\n",
    "    # so that can perform operations over one minute / one second intervals\n",
    "    number_of_one_minute_intervals = int( np.floor( len(data_raw) / one_minute_in_datapoints ))\n",
    "    data1 = data_raw[0: number_of_one_minute_intervals * one_minute_in_datapoints] #step 1 of cleaning\n",
    "    \n",
    "    # Remove median of whole dataset\n",
    "    data2 = data1 - np.median(data1) #step 2 of cleaning\n",
    "    \n",
    "    # Median removal over 1 second intervals (not a proper filter with sliding window!)\n",
    "    median_filter_1s = np.zeros(len(data2))\n",
    "    number_of_one_second_intervals = number_of_one_minute_intervals * 60\n",
    "\n",
    "    second_long_chunks_starting_points = np.linspace(0, len(data2) - one_second_in_datapoints , number_of_one_second_intervals , dtype='int')\n",
    "\n",
    "    for second_long_chunk_start in second_long_chunks_starting_points :\n",
    "        second_long_chunk_end =  second_long_chunk_start + one_second_in_datapoints\n",
    "        \n",
    "        median_filter_1s[second_long_chunk_start : second_long_chunk_end] = np.median(data2[second_long_chunk_start : second_long_chunk_end]) \n",
    "\n",
    "    data3 = data2 - median_filter_1s #step 3 of cleaning\n",
    "\n",
    "    # Sigma clip over 1 minute intervals with a median centre function\n",
    "    data4 = np.zeros(len(data3))\n",
    "    \n",
    "    minute_long_chunks_starting_points = np.linspace(0, len(data3) - one_minute_in_datapoints , number_of_one_minute_intervals , dtype='int')\n",
    "    \n",
    "    for minute_long_chunk_start in minute_long_chunks_starting_points :\n",
    "        minute_long_chunk_end =  minute_long_chunk_start + one_minute_in_datapoints\n",
    "        \n",
    "        sigma_masked_array = sigma_clip(data3[minute_long_chunk_start : minute_long_chunk_end], sigma=5, cenfunc='median', masked=True) #this is a NumPy MaskedArray object\n",
    "        sigma_masked_array.set_fill_value(0.0)\n",
    "        \n",
    "        data4[minute_long_chunk_start : minute_long_chunk_end] = sigma_masked_array.filled()\n",
    "        \n",
    "    # STD divide to whiten data over a 1 minute interval\n",
    "        data_cleaned = np.zeros(len(data4))\n",
    "        \n",
    "        for minute_long_chunk_start in minute_long_chunks_starting_points :\n",
    "    \n",
    "            minute_long_chunk_end =  minute_long_chunk_start + one_minute_in_datapoints\n",
    "            \n",
    "            if np.std(data4[minute_long_chunk_start : minute_long_chunk_end]) != 0 :\n",
    "                data_cleaned[minute_long_chunk_start : minute_long_chunk_end] = data4[minute_long_chunk_start : minute_long_chunk_end]/np.std(data4[minute_long_chunk_start : minute_long_chunk_end]) #4th and last step of cleaning\n",
    "        \n",
    "    # Write the cleaned file in the same way as a raw file, so can be opened the same way\n",
    "    handle_file_cleaned = open(path_to_cleaned_files+str(start_time_GPS)+'_cleaned.dat', 'wb')\n",
    "    data_cleaned_binary = np.array(data_cleaned, 'f4')\n",
    "    data_cleaned_binary.tofile(handle_file_cleaned)\n",
    "    \n",
    "    # Close files\n",
    "    handle_file_cleaned.close()\n",
    "    handle_raw_file.close()\n",
    "\n",
    "\n",
    "bar.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Folding, SNR and TOA computations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the number of phase bins in the folded profiles and in the total profile, it has to be a power of two, for FFTFIT.\n",
    "# Otherwise, for folding, PRESTO defaults to the number of sampling bins which correspond to one folded period, in our case about 64, and for the total profile, it defaults to 128.\n",
    "number_of_profile_bins = '512' #I chose this amount to limit computation time with a good precision still \n",
    "\n",
    "# Start a log\n",
    "log_handle = open('PRESTO_Fold_SNR_TOA.log', 'w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find out time span of the available observations (for information and context only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_files_paths = sorted(glob.glob(path_to_cleaned_files +'*_cleaned.dat'))\n",
    "minimum_GPS_time = Time(float(cleaned_files_paths[0][len(path_to_cleaned_files):-len('_cleaned.dat')]), format='gps', scale='utc') #this is the first date at which I start having observations, extracted from the first file name of the ordered cleaned files.\n",
    "log_handle.write('The observations start on ' + minimum_GPS_time.iso+'\\n')\n",
    "maximum_GPS_time = Time(float(cleaned_files_paths[len(cleaned_files_paths)-1][len(path_to_cleaned_files):-len('_cleaned.dat')]), format='gps', scale='utc') #this is the last observation date\n",
    "log_handle.write('The observations end on ' + maximum_GPS_time.iso+'\\n')\n",
    "observations_span = maximum_GPS_time - minimum_GPS_time\n",
    "log_handle.write('The observations span '+str(observations_span.jd)+' days. \\n')\n",
    "log_handle.write('\\n \\n \\n \\n') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRESTO_TOA_SNR_handle = open('PRESTO_TOAs_SNRs.txt' , 'w') #create a new file for saving PRESTO's TOA and SNR output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pulsar topocentric frequency predictions, folding, SNR and TOA computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[KProcessing... |游니游니游니游니游니游니游니游니游니游니                      | 34.1% - 876s"
     ]
    }
   ],
   "source": [
    "bar = Bar('Processing...', max=len(cleaned_files_paths), fill='\\U0001F4E1', suffix = '%(percent).1f%% - %(eta)ds') #create a progress bar\n",
    "bar.check_tty = False\n",
    "\n",
    "for cleaned_file_path in cleaned_files_paths:\n",
    "    \n",
    "    # Find out the start time and length of the observation\n",
    "    \n",
    "    start_time_GPS = float(cleaned_file_path[len(path_to_cleaned_files):-len('_cleaned.dat')])\n",
    "    start_time_GPS_astropy = Time(start_time_GPS, format='gps', precision=6, scale='utc') #important step! \n",
    "    log_handle.write('Observation starting on GPS time '+str(start_time_GPS)+' i.e. '+str(start_time_GPS_astropy.iso)+'\\n')\n",
    "    handle_cleaned_file = open(cleaned_file_path)\n",
    "    data_cleaned = np.fromfile(handle_cleaned_file,'f4') #cleaned dataset\n",
    "    total_seconds_cleaned = len(data_cleaned)*sampling_period #total seconds in dataset\n",
    "    total_hours_cleaned =  total_seconds_cleaned / 3600 #total hours in dataset\n",
    "    total_hours_rounded_cleaned = np.ceil(total_hours_cleaned) #total rounded hours in dataset\n",
    "    \n",
    "    log_handle.write(' \\n ') \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    # Generate the TEMPO polynomial coefficients for this observation\n",
    "    \n",
    "    # Create the custom TEMPO command for this file\n",
    "    TEMPO_command = 'tempo -ZOBS=AR -ZFREQ=407.5 -ZTOBS='+str(total_hours_rounded_cleaned)+' -ZSTART='+str(start_time_GPS_astropy.mjd)+'  -ZNCOEFF=15 -ZSPAN='+str(int(total_hours_rounded_cleaned))+'H -f J0332+5434_initial_parameters.par'\n",
    "    log_handle.write('TEMPO command: ' +TEMPO_command+'\\n')\n",
    "\n",
    "    terminal_TEMPO_run = subprocess.run(TEMPO_command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "    \n",
    "    log_handle.write(terminal_TEMPO_run.stdout.decode('utf-8')+'\\n') #copy the TEMPO output to the log\n",
    "    \n",
    "    # Here I add the name of the pulsar because for some reason, TEMPO does not put it in, and it is needed in PRESTO\n",
    "    polyco_handle = open('polyco.dat','r+')\n",
    "    polyco_contents = polyco_handle.read()\n",
    "    polyco_handle.seek(0,0)\n",
    "    polyco_handle.write('0332+5434  ')\n",
    "    polyco_handle.close()\n",
    "    \n",
    "    # When doing TOA finding later in this for loop, PRESTO looks for the polycos along with the folded profile\n",
    "    copyfile('polyco.dat', path_to_folded_profiles+str(start_time_GPS)+'_PSR_0332+5434.pfd.polycos')\n",
    "\n",
    "    \n",
    "    log_handle.write(' \\n ') \n",
    "    \n",
    "    \n",
    "   \n",
    "\n",
    "    # Create PRESTO .inf file for this observation\n",
    "    # This file contains information about the observation essential to the PRESTO folding software\n",
    "    # Modify a template inf file. It has empty slots that need filled.\n",
    "    \n",
    "    copyfile('Template_PRESTO_inf_file.txt', 'current_PRESTO_inf_file.inf') #this creates an empty template file, and overwrites the previous instance of it\n",
    "    \n",
    "    # Write the datafile name, without suffix\n",
    "    update_text(filename='current_PRESTO_inf_file.inf', lineno=1, column=44, text=cleaned_file_path[:-len('.dat')])\n",
    "    \n",
    "    # Write the observation start MJD\n",
    "    update_text(filename='current_PRESTO_inf_file.inf', lineno=8, column=44, text=str(start_time_GPS_astropy.mjd))\n",
    "    \n",
    "    # Write the number of bins in the time series\n",
    "    update_text(filename='current_PRESTO_inf_file.inf', lineno=10, column=44, text=str(len(data_cleaned)))\n",
    "    \n",
    "    # PRESTO will look for this information file in the same location as the data file, with the same name. This will overwrite any previous instances.\n",
    "    copyfile('current_PRESTO_inf_file.inf', cleaned_file_path[:-len('dat')]+'inf')\n",
    "    # PRESTO also looks for an inf file along with the folded profiles later, when doing TOA finding\n",
    "    copyfile('current_PRESTO_inf_file.inf', path_to_folded_profiles+str(start_time_GPS)+'_PSR_0332+5434.pfd.inf')\n",
    "    # Note that it does not find it. I tried naming it without the pfd, or the same as the one for the cleaned file, but no luck. But that's okay as you will see later.\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Perform the fold for this observation\n",
    "    \n",
    "    PRESTO_fold_command = 'prepfold -nosearch -polycos polyco.dat -psr 0332+5434 -double -noxwin -n '+number_of_profile_bins+' -o '+path_to_folded_profiles+str(start_time_GPS)+' '+cleaned_file_path\n",
    "    log_handle.write('PRESTO fold command: ' + PRESTO_fold_command+'\\n')\n",
    "    \n",
    "    terminal_fold_run = subprocess.run(PRESTO_fold_command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "    log_handle.write(terminal_fold_run.stdout.decode('utf-8')+'\\n') #copy the PRESTO prepfold output to the log\n",
    "    \n",
    "    log_handle.write(' \\n ')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Compute TOA, TOA error and SNR using FFTFIT\n",
    "    PRESTO_TOA_SNR_command = 'get_TOAs.py -f -n 1 -g '+path_to_template_profile+' '+path_to_folded_profiles+str(start_time_GPS)+'_PSR_0332+5434.pfd'\n",
    "    \n",
    "    log_handle.write('PRESTO TOA command: ' + PRESTO_TOA_SNR_command+'\\n')\n",
    "    log_handle.write('Output in PRESTO_TOAs_SNRs.txt . \\n')\n",
    "    \n",
    "    terminal_TOA_SNR_run = subprocess.run(PRESTO_TOA_SNR_command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "    \n",
    "    PRESTO_TOA_SNR_handle.write(terminal_TOA_SNR_run.stdout.decode('utf-8')+'\\n') #copy PRESTO get_TOAs.py output to log\n",
    "    # I currently get a warning saying that the .inf file cannot be found. I wonder why! I have tried naming it different things. It's OK, because the information taken from the inf file in the case of TOA finding defaults to the right values: DM 0 and number of channels 1.\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    log_handle.write('\\n \\n \\n \\n \\n')\n",
    "    bar.next()\n",
    "\n",
    "bar.finish()\n",
    "time.sleep(2) #let the bar close up properly\n",
    "print('Polycos, folding and retrieving TOAs took '+str(bar.elapsed-2)+' s.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Post-loop cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove('current_PRESTO_inf_file.inf') #inf file of the last processed observation\n",
    "os.remove('polyco.dat') #this was the predictor file of the last processed observation\n",
    "PRESTO_TOA_SNR_handle.close() #close the file, as it was open for writing and now we want to read it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract the SNR and TOAs (with error) from the PRESTO output and save them into separate files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRESTO_TOA_SNR_handle = open('PRESTO_TOAs_SNRs.txt' , 'r')\n",
    "TOAs_handle = open('TOAs_TEMPO_format.txt', 'w') #new file\n",
    "FFTFIT_results_handle = open('FFTFIT_results.txt', 'w') #new file\n",
    "\n",
    "# To find floats, integers, and numbers with exponents in output files, I need to use regular expressions\n",
    "# This one was taken from https://stackoverflow.com/questions/4703390/how-to-extract-a-floating-number-from-a-string , second answer\n",
    "regexp_numeric_pattern = r'[-+]? (?: (?: \\d* \\. \\d+ ) | (?: \\d+ \\.? ) )(?: [Ee] [+-]? \\d+ ) ?'\n",
    "# This expression needs compiled by regexp\n",
    "any_number = re.compile(regexp_numeric_pattern, re.VERBOSE)\n",
    "\n",
    "\n",
    "for line in PRESTO_TOA_SNR_handle:\n",
    "    if 'SNR' in line:\n",
    "        b, b_error, SNR, SNR_error, _, _, _  = any_number.findall(line) \n",
    "        FFTFIT_results_handle.write(b+' '+b_error+' '+SNR+' '+SNR_error+'\\n')\n",
    "    if 'a                407.500' in line: #a is the TEMPO one-letter code from obsys.dat\n",
    "        TOAs_handle.write(line)\n",
    "\n",
    "# Close files to read them rather than write\n",
    "TOAs_handle.close()       \n",
    "PRESTO_TOA_SNR_handle.close()\n",
    "FFTFIT_results_handle.close()\n",
    "\n",
    "\n",
    "TOAs_with_error = np.genfromtxt('TOAs_TEMPO_format.txt')[:,2:4]\n",
    "FFTFIT_results = np.genfromtxt('FFTFIT_results.txt')\n",
    "\n",
    "#Because floating points are inaccurate, I can't seem to add a column of TOAs/MJDs to my FFTFIT results without removing precision. If you still want to add them, go ahead:\n",
    "# =============================================================================\n",
    "# FFTFIT_results_with_MJDs =  np.c_[TOAs_with_error[:,0] , FFTFIT_results]\n",
    "# np.savetxt('FFTFIT_results.txt', FFTFIT_results_with_MJDs, fmt='%f')\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot SNR and TOA results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SNR_list = FFTFIT_results[:,2]\n",
    "average_SNR = np.mean(SNR_list)\n",
    "log_handle.write('The average SNR is '+str(average_SNR)+'\\n')\n",
    "\n",
    "average_TOA_error = np.mean(TOAs_with_error[:,1])\n",
    "log_handle.write('The average TOA error bar is '+str(average_TOA_error)+' microseconds.\\n')\n",
    "\n",
    "fig1 = plt.figure()\n",
    "ax1 = plt.gca()\n",
    "ax1.plot(TOAs_with_error[:,0], SNR_list, marker='o', color='black', linestyle='none')\n",
    "ax1.set_xlabel('TOA (MJD)')\n",
    "ax1.set_ylabel('SNR')\n",
    "plt.savefig('SNR_vs_TOA.pdf') \n",
    "plt.close()\n",
    "\n",
    "fig2 = plt.figure()\n",
    "ax2 = plt.gca()\n",
    "ax2.plot(SNR_list, TOAs_with_error[:,1],  marker='o', color='black', markersize=2, linestyle='none')\n",
    "ax2.plot(np.linspace(0.4,12.7,num=100), (sampling_period*1e6)/np.linspace(0.4,12.7,num=100), linewidth=0.5, color='red', label='Ratio of sampling period to SNR')\n",
    "ax2.set_xlabel('SNR')\n",
    "ax2.set_ylabel('TOA error bar ($\\mu$s)')\n",
    "ax2.legend()\n",
    "plt.savefig('TOA_errors_vs_SNRs.pdf') \n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_handle.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pulsar timing fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the first observed TOA to the model as a reference for zero phase (with error)\n",
    "# This is a standard procedure\n",
    "# TZRMJD = Zero Timing Residual MJD\n",
    "lines = fileinput.input(files='J0332+5434_initial_parameters.par', inplace=1) #read the lines of the initial parameters file\n",
    "for line in lines:\n",
    "    if 'TZRMJD' in line:\n",
    "        line = 'TZRMJD          '+str(TOAs_with_error[0,0])+'            '+str(TOAs_with_error[0,1]) #replace TZRMJD with the latest first TOA\n",
    "    print(line, end = '')\n",
    "lines.close()\n",
    "\n",
    "# Import model\n",
    "model = models.model_builder.get_model('J0332+5434_initial_parameters.par')\n",
    "\n",
    "# Import TOAs\n",
    "TOAs = toa.get_TOAs('TOAs_TEMPO_format.txt', planets=True)\n",
    "\n",
    "# Apply SNR cutoff\n",
    "SNR_cutoff = 4\n",
    "SNR_list_masked = np.ma.masked_where(SNR_list<SNR_cutoff, SNR_list)\n",
    "TOAs.select(~SNR_list_masked.mask)\n",
    "TOAs.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-fit residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefit_residuals = residuals.Residuals(TOAs, model)\n",
    "\n",
    "# Plot them\n",
    "fig3 = plt.figure()\n",
    "ax3 = plt.gca()\n",
    "ax3.errorbar(\n",
    "    TOAs.get_mjds().value,\n",
    "    prefit_residuals.time_resids.to(units.us),\n",
    "    yerr=TOAs.get_errors().to(units.us),\n",
    "    marker='o',markersize=0.5,\n",
    "    color='black', elinewidth=0.5, capsize=0, linestyle='none')\n",
    "ax3.set_title(\"%s Pre-fit Timing Residuals\" % model.PSR.value)\n",
    "ax3.set_xlabel(\"MJD\")\n",
    "ax3.set_ylabel(\"Residual ($\\mu$s)\")\n",
    "plt.savefig('Pre_fit_residuals.pdf') \n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting and post-fit residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the fit\n",
    "WLS_fit = fitter.WLSFitter(TOAs, model)\n",
    "WLS_fit.set_fitparams('F0','F1', 'RAJ', 'DECJ') \n",
    "WLS_fit.fit_toas()\n",
    "\n",
    "fig4 = plt.figure()\n",
    "ax4 = plt.gca()\n",
    "\n",
    "# Plot the post-fit residuals\n",
    "ax4.errorbar(TOAs.get_mjds().value, WLS_fit.resids.time_resids.to(units.us), yerr=TOAs.get_errors().to(units.us), color='black', elinewidth=0.05, capsize=0, marker='o',markersize=0.8, linestyle='none')\n",
    "#ax4.scatter(TOAs.get_mjds().value, WLS_fit.resids.time_resids.to(units.us), marker='.', color='black')\n",
    "ax4.set_title(\"%s Post-Fit Timing Residuals\" % model.PSR.value)\n",
    "ax4.set_xlabel(\"MJD\")\n",
    "ax4.set_ylabel(\"Residual ($\\mu$s)\")\n",
    "plt.savefig('Post_fit_residuals.pdf')\n",
    "#plt.savefig('Post_fit_residuals_no_error_bars.pdf') \n",
    "plt.close()\n",
    "\n",
    "# Write the updated timing model \n",
    "model_handle = open('J0332+5434_post-fit_parameters.par', 'w')\n",
    "model_handle.write(WLS_fit.model.as_parfile())\n",
    "model_handle.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total summed profile "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  /!\\ /!\\ Before this is done, you must run the folding, SNR and TOA computations with the updated model, in absolute phase. To do so, add '-absphase' and replace the model name 'J0332+5434_initial_parameters.par' to 'J0332+5434_post-fit_parameters.par' in the PRESTO command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the template profile\n",
    "template_profile = np.genfromtxt(path_to_template_profile)\n",
    "phase_bins = template_profile[:,0]\n",
    "template_profile =  template_profile[:,1]/np.max(template_profile)\n",
    "\n",
    "\n",
    "# List the profiles to add, in chronological order\n",
    "profile_files_paths = np.array(sorted(glob.glob(path_to_folded_profiles+'*.bestprof')))\n",
    "\n",
    "\n",
    "SNR_cutoffs = np.linspace(1,7,25) #create a range of SNR cutoffs which will select profiles to be added in the total profile\n",
    "mean_residuals = []\n",
    "max_residuals = []\n",
    "\n",
    "# Loop through cutoffs\n",
    "for SNR_cutoff in SNR_cutoffs:\n",
    "    \n",
    "    total_profile = np.zeros(512) #empty the total profile\n",
    "    \n",
    "    # Apply the cutoff\n",
    "    for profile_file_path, SNR in zip(profile_files_paths[SNR_list>SNR_cutoff] , SNR_list[SNR_list>SNR_cutoff]):\n",
    "        profile = np.genfromtxt(profile_file_path, skip_header=26)[:,1]\n",
    "        \n",
    "        total_profile += profile*SNR #add the weighted profile\n",
    "        \n",
    "    # Convert the total profile to a format compatible with the template    \n",
    "    total_profile = np.roll(total_profile, int(len(total_profile)/2))\n",
    "    total_profile -= np.median(total_profile)\n",
    "    total_profile =  resample(total_profile, len(template_profile))\n",
    "    total_profile =  total_profile/np.max(total_profile)\n",
    "    \n",
    "    residuals = template_profile - total_profile\n",
    "    mean_residuals.append(np.mean(residuals))\n",
    "    max_residuals.append(np.max(residuals))\n",
    "    \n",
    "\n",
    "    fig1 = plt.figure()\n",
    "    ax1 = plt.gca()\n",
    "    plt.step(phase_bins, total_profile, linewidth=0.5, color='black', label='Total Profile')\n",
    "    plt.step(phase_bins, template_profile, linewidth=0.5, color='red', label='Template Profile')\n",
    "    plt.legend()\n",
    "    ax1.set_title('Total profile comparison SNR cutoff '+str(SNR_cutoff))\n",
    "    ax1.set_xlabel(\"Phase\")\n",
    "    ax1.set_ylabel(\"Relative intensity\")\n",
    "    plt.savefig('Total_Profile_Comparison_SNR_cutoff_'+str(SNR_cutoff)+'.pdf') \n",
    "    plt.close()\n",
    "\n",
    "\n",
    "fig2, ax2 = plt.subplots(2,1, figsize=(11,10))\n",
    "ax2[0].plot(SNR_cutoffs, mean_residuals, marker='o', markersize=3, linestyle='none', color='black')\n",
    "ax2[0].set_ylabel(\"Mean residuals (relative intensity)\")\n",
    "ax2[1].plot(SNR_cutoffs, max_residuals, marker='o', markersize=3, linestyle='none', color='black')\n",
    "ax2[1].set_xlabel(\"SNR cutoff\")\n",
    "ax2[1].set_ylabel(\"Maximum residuals (relative intensity)\")\n",
    "fig2.suptitle('Evolution of residuals with SNR cutoff')\n",
    "plt.savefig('Evolution_of_total_profile_residuals_with_SNR.pdf') \n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now choose the best cutoff.\n",
    "### With this new cutoff you can repeat the PINT timing and update your model.\n",
    "### Then you can perform this total profile again, with the chosen cutoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
